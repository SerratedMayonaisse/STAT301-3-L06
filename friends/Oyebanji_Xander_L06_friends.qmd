---
title: "Friends Dialogue Bonus"
subtitle: "Predicting the Season of Friends from the Dialogue"
author: "Xander Oyebanji"
pagetitle: "L06 Xander Oyebanji"
date: today

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    code-fold: false
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji
reference-location: margin
citation-location: margin
---


::: {.callout-tip icon=false}

## Github Repo Link
THIS LAB WAS DONE OVER 2 REPOSITORIES BECAUSE OF TECHNICAL ISSUES!
The link for the original repository that could not be salvaged was:
[https://github.com/stat301-3-2024-spring/L06-nlp-SerratedMayonaisse.git](https://github.com/stat301-3-2024-spring/L06-nlp-SerratedMayonaisse.git)

The link for the current repository is this:
[https://github.com/SerratedMayonaisse/STAT301-3-L06.git](https://github.com/SerratedMayonaisse/STAT301-3-L06.git)

:::

## Goal
  The goal of this lab is to predict what episode of the TV show `Friends` a certain bit of dialogue is from using predictive modeling and natural language processing (NLP).
  
# Data 
  The dataset for this lab contains dialogue from episodes of `Friends`, along with the season, episode, scene, and character that said the line. This dataset is quite large, with over 67,000 observations. For the most part, missingness was not an issue, with the only variable with any missingness issues being the character that said the line. This is not a problem because it will not be used in the prediction problem.
  
# Model Background
  The models that were used for this prediction problem were a null model and a naive bayes model. These models were chosen because of their quick processing time. Tokenization and TF IDF were used in the recipe for the models in order to allow for NLP to work. Models were trained on folded data, as well as the whole training dataset so that they could be tested on the testing dataset.
  
## Training Results
```{r}
#| label: loading packages
#| echo: false
#| results: false
library(tidyverse)
library(here)
library(tidymodels)
#handling common conflicts
tidymodels_prefer()
```
```{r}
#| label: metrics from trained models
#| echo: false
load(here("friends/figures/training_metrics.rda"))
training_metrics
```
  Looking at the models trained on folded data (not predicting testing data), it can be seen that the null model has an average accuracy of 0.106 while the naive bayes model has an average accuracy of 0.111. Because the accuracy is higher on the folded training data, it is expected that the naive bayes model will perform slightly better on the actual testing data. However, in terms of brier class, the null model has a much lower value of 0.450 compared to 0.877 of the naive bayes model. By this metric, it is suggested that the null model will actually perform better than the naive bayes model when they are used on the testing data. Finally, in terms of ROC AUC, the naive bayes model performed better, with a ROC AUC of 0.530 compared to the null model's 0.500. Higher ROC AUC values are better, so this indicates that the naive bayes model will perform better on the testing data.
  Despite these conflicting metrics, 2 of the 3 indicate that the naive bayes model will perform better, so that seems like the more likely option.
```{r}
#| label: conformation matrix null
#| echo: false
load(here("friends/figures/conf_mat_null.rda"))
conf_mat_null
```
  Looking at the conformation matrix of the predictions the null model made on the training data, it seems that it resorted to only choosing two of the 10 categories, 3 or 6. This plot indicates that the models were likely switching between 3 or 6 every time and sticking to one, given that's how null models work. Because of the guesses being split pretty evenly between the two and there being a similar number of episodes in each season in the data, very few of these predictions were actually correct.
```{r}
#| label: conformation matrix naive bayes
#| echo: false
load(here("friends/figures/nb_conf_mat.rda"))
nb_conf_mat
```
  Compared to the null model, the naive bayes model had much better distributed predictions, though most of the predictions were focused on the 6th season. Most of the guesses of season 6 were wrong, but the most populated category of 6th season guesses was a true positive, indicating some level of actual predictions going on. This is weaker in some rows than others, for example, in some rows the true positive answer is one of the less guessed categories, but it is true in a number of categories. Overall, this model shows better promise than the null model, but is still incredibly weak.
  According to the metrics and a look at the conformation matrices of the null model and naive bayes model on the folded training data, it seems like the naive bayes model will perform slightly better on the testing data than the null model.
  
## Testing Results
```{r}
#| label: final accuracies
#| echo: false
load(here("friends/figures/final_accuracy.rda"))
final_null_accuracy
```
  After running the models on the testing data, it appears that, according to their accuracies, the naive bayes model performed slightly better than the null model with an accuracy of 0.109 compared to the null model's accuracy of 0.108. This lines up with what was expected. Neither of these accuracies are high enough to be considered acceptable for use.
```{r}
#| label: conformation matrix null final
#| echo: false
load(here("friends/figures/final_null_conf_mat.rda"))
final_null_conf_mat
```
Looking at what the null model did in the conformation matrix, it appears that like with the training data, it only guessed one value the whole time (which makes sense since it's a null model). Due to there being 10 seasons with similar numbers of episodes with similar numbers of dialogue, the observations are pretty evenly split into each of the 10 seasons. This means that the accuracy that was achieved, a little over 10 percent, is about what could be expected. Even with a further developed recipe, this model shows little promise and will likely always get around the same accuracy of about 10% due to it sticking to one value.
```{r}
#| label: conformation matrix naive bayes final
#| echo: false
load(here("friends/figures/final_nb_conf_mat.rda"))
final_nb_conf_mat
```
  The naive bayes model once again mainly stuck to season 6, with a few guesses elsewhere. Due to being able to make predictions outside of just one value, it makes sense that it performed better than the null model. Everything about this conformation matrix is in line with what the previous conformation matrix for the naive bayes model would indicate.
  In conclusion, the naive bayes model performed slightly better in terms of accuracy and appeared to be making better types of guesses than the null model, which only predicted one value, indicating it was the better model. However, neither of these models are suitable prediction models. If this lab were to be done again, a different model type that is not a baseline model type would be more suitable due to higher complexity allowing for more accurate predictions and the ability to tune them to find optimal hyperparameters. Additionally, combining different types of model types in an ensemble model would likely further improve the ability to predict the model's predictive capabilities. Most importantly, a more complex recipe is needed in order to make accurate predictions. Currently, the recipe is quite bare and could be benefited from additional steps and tuning.

